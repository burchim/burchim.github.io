<html>
    <head>
        <title>Maxime Burchi</title>
        <link rel="stylesheet" type="text/css" href="css/style.css">
        <link rel="stylesheet" type="text/css" href='css/bootstrap.min.css'>
        <style>
            body {
                font-family: 'sans-serif';
                font-size: 14pt;
                background-color: #FFFCF4;
                color: #4F6071;
            }
        </style>
    </head>
    <body>
        <div id="header" class="bg">
            <div id='header-inner'>
                <img src='images/me.jpg' class='img-me'>
                <div class='header-text'>
                    <div class='header-text-name'>
                        Maxime Burchi
                    </div>
                    <div class='header-text-email'>
                        maxime.burchi [at] uni-wuerzburg.de
                    </div>
                    <div>
                        <a href="https://github.com/burchim" class="header-text-link">[GitHub]</a>
                        <a href="https://scholar.google.com/citations?user=7S_l2eAAAAAJ&hl=en" class="header-text-link">[Google Scholar]</a>
                        <a href="https://www.linkedin.com/in/maxime-burchi-5566a71a2/" class="header-text-link">[LinkedIn]</a>
                        <a href="pdf/CV_MaximeBurchi.pdf" class="header-text-link">[CV]</a>
                    </div>
                </div>
            </div>
        </div>

        <div class='container'>
            <div class='col-xs-10 col-md-offset-1'>

                <div class='row'>
                    <h1>About</h1>
                    <p>I am a PhD student at the University of Würzburg, Germany, working under the supervision of <a href="https://people.ee.ethz.ch/~timofter/">Radu Timofte</a> (University of Würzburg, ETH Zurich). My research interests include Deep Learning, Computer Vision, Spoken Language Processing, Natural Language Processing and Reinforcement Learning.</p>
                    <p>I am currently working on Multimodal Learning and Model-based Reinforcement Learning. More specifically, the learning of predictive world models and controllable generative models from videos, allowing reinforcement learning agents to learn from imaginary situations without interacting with the real environment.</p>
                    <p>Before my PhD, I graduated from l'ESIEE Paris where I received a Master of Engineering degree and completed a graduation internship at Orange where I proposed an efficient solution for large vocabulary Automatic Speech Recognition.</p>
                </div>

                <div class='row'>
                    <hr>
                </div>

                <div class='row'>
                    <h1>News</h1>
                    <ul>
                        <li><b>05 / 2025</b> One paper accepted at ICML 2025</li>
                        <li><b>01 / 2025</b> One paper accepted at ICLR 2025</li>
                        <li><b>12 / 2023</b> One paper accepted at ICASSP 2024</li>
                        <li><b>09 / 2023</b> Completed a 6-month research internship at Nvidia in Paris</li>
                        <li><b>10 / 2022</b> One paper accepted at WACV 2023</li>
                        <li><b>03 / 2022</b> Started my PhD and joined the Computer Vision Laboratory at the University of Würzburg</li>
                        <li><b>09 / 2021</b> One paper accepted at ASRU 2021</li>
                        <li><b>08 / 2021</b> Completed a 6-month research internship at Orange in Rennes</li>
                    </ul>
                </div>

                <div class='row'>
                    <hr>
                </div>

                <div class='row'>
                    <h1>Publications</h1>

                    <div class='row vspace-top'>
                        <div class='col-xs-3 text-center'>
                            <img class='paper-image' src='images/EMERALD.png'>
                        </div>
                        <div class='col-xs-9'>
                            <div class='paper-title'>
                                Accurate and Efficient World Modeling with Masked Latent Transformers
                            </div>
                            <div class='paper-authors'>
                                <u>Maxime Burchi</u>, Radu Timofte
                            </div>
                            <div>
                                ICML 2025
                            </div>
                            <div>
                                <a href='https://openreview.net/forum?id=zNUOZcAUxz'>[OpenReview]</a>
                                <a href='https://www.arxiv.org/abs/2507.04075'>[arXiv]</a>
                                <a href='https://github.com/burchim/EMERALD'>[code]</a>
                            </div>
                        </div>
                    </div>

                    <div class='row vspace-top'>
                        <div class='col-xs-3 text-center'>
                            <img class='paper-image' src='images/TWISTER.jpg'>
                        </div>
                        <div class='col-xs-9'>
                            <div class='paper-title'>
                                Learning Transformer-based World Models with Contrastive Predictive Coding
                            </div>
                            <div class='paper-authors'>
                                <u>Maxime Burchi</u>, Radu Timofte
                            </div>
                            <div>
                                ICLR 2025
                            </div>
                            <div>
                                <a href='https://openreview.net/forum?id=YK9G4Htdew'>[OpenReview]</a>
                                <a href='https://www.arxiv.org/abs/2503.04416'>[arXiv]</a>
                                <a href='https://github.com/burchim/TWISTER'>[code]</a>
                            </div>
                        </div>
                    </div>

                    <div class='row vspace-top'>
                        <div class='col-xs-3 text-center'>
                            <img class='paper-image' src='images/MuDreamer.jpg'>
                        </div>
                        <div class='col-xs-9'>
                            <div class='paper-title'>
                                MuDreamer: Learning Predictive World Models without Reconstruction
                            </div>
                            <div class='paper-authors'>
                                <u>Maxime Burchi</u>, Radu Timofte
                            </div>
                            <div>
                                <a href='https://arxiv.org/abs/2405.15083'>[arXiv]</a>
                                <a href='https://github.com/burchim/MuDreamer'>[code]</a>
                            </div>
                        </div>
                    </div>

                    <div class='row vspace-top'>
                        <div class='col-xs-3 text-center'>
                            <img class='paper-image' src='images/AVFC.jpg'>
                        </div>
                        <div class='col-xs-9'>
                            <div class='paper-title'>
                                Multilingual Audio-Visual Speech Recognition with Hybrid CTC/RNN-T Fast Conformer
                            </div>
                            <div class='paper-authors'>
                                <u>Maxime Burchi</u>, Krishna C. Puvvada, Jagadeesh Balam, Boris Ginsburg, Radu Timofte
                            </div>
                            <div>
                                ICASSP 2024
                            </div>
                            <div>
                                <a href='https://www.arxiv.org/abs/2405.12983'>[arXiv]</a>
                                <a href='https://github.com/NVIDIA/NeMo'>[code]</a>
                            </div>
                        </div>
                    </div>

                    <div class='row vspace-top'>
                        <div class='col-xs-3 text-center'>
                            <img class='paper-image' src='images/AVEC.jpg'>
                        </div>
                        <div class='col-xs-9'>
                            <div class='paper-title'>
                                Audio-Visual Efficient Conformer for Robust Speech Recognition
                            </div>
                            <div class='paper-authors'>
                                <u>Maxime Burchi</u>, Radu Timofte
                            </div>
                            <div>
                                WACV 2023
                            </div>
                            <div>
                                <a href='https://arxiv.org/abs/2301.01456'>[arXiv]</a>
                                <a href='https://github.com/burchim/AVEC'>[code]</a>
                            </div>
                        </div>
                    </div>

                    <div class='row vspace-top'>
                        <div class='col-xs-3 text-center'>
                            <img class='paper-image' src='images/EfficientConformer.jpg'>
                        </div>
                        <div class='col-xs-9'>
                            <div class='paper-title'>
                                Efficient conformer: Progressive downsampling and grouped attention for automatic speech recognition
                            </div>
                            <div class='paper-authors'>
                                <u>Maxime Burchi</u>, Valentin Vielzeuf
                            </div>
                            <div>
                                ASRU 2021
                            </div>
                            <div>
                                <a href='https://arxiv.org/abs/2109.01163'>[arXiv]</a>
                                <a href='https://github.com/burchim/EfficientConformer'>[code]</a>
                            </div>
                        </div>
                    </div>

                </div>

                <div class='row vspace-top'></div>

            </div>
        </div>
    </body>
</html>